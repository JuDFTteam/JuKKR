\documentclass [a4paper, 12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{times}
\usepackage{float}
\usepackage[usenames,dvipsnames]{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cleveref}

\textwidth 16 cm
\textheight 23 cm
\setlength{\oddsidemargin}{0.1 cm}
\setlength{\topmargin}{1 cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}
\setlength{\footskip}{0.75cm}
\setlength{\parindent}{0cm}
\setlength{\oddsidemargin}{0.1 cm}
\setlength{\itemsep}{10pt}
\bibliographystyle{gcs}

\DeclareUnicodeCharacter{2212}{-}
\newcommand{\matr}[1]{\bm{#1}}  

\begin{document}
 
\begin{figure}[H]
\begin{center}
  \includegraphics[scale=0.45]{Figures/GCS-hlrs-fzj-lrz.jpg}\\
\end{center}
\end{figure}

\begin{center}
{\LARGE \bf Project Proposal for Tier0/Tier1 HPC Access} \\

\bigskip
\bigskip
\bigskip
\end{center}
\textbf{Period}\\
\phantom{MM}\textit{01.11.2017-31.10.2018}

\bigskip
\textbf{Project title}\\
\phantom{MM}\textit{KKRnano: Quantum description of topological matter}

\bigskip
\textbf{Type of project}\\
\phantom{MM} \textit{new project}

%\bigskip
%\textbf{Project ID}\\
%\phantom{MM} \textit{Please provide in case of a project extension}

\bigskip
\textbf{Principal investigator}\\
\phantom{MM} \textit{ Prof. Dr. Stefan Bl{\"u}gel,
Institute for Advanced Simulation, Forschungszentrum J\"ulich, D-52425 J\"ulich, Germany
}

\bigskip
\textbf{Project contributor(s)}\\

\phantom{MM} \textit{Marcel Bornemann,
Institute for Advanced Simulation, Forschungszentrum J\"ulich, D-52425 J\"ulich, Germany
}

\phantom{MM} \textit{Dr. Rudolf Zeller,
Institute for Advanced Simulation, Forschungszentrum J\"ulich, D-52425 J\"ulich, Germany
}

\phantom{MM} \textit{Dr. Paul F. Baumeister,
J\"ulich Supercomputing Centre, Forschungszentrum J\"ulich, D-52425 J\"ulich, Germany
}


\phantom{MM} \textit{Prof. Dr. Peter H. Dederichs,
Institute for Advanced Simulation, Forschungszentrum J\"ulich, D-52425 J\"ulich, Germany
}



\newpage

\vfill
\tableofcontents
\vfill

\newpage



\section{Introduction}
\rule{\textwidth}{0.4pt}\\
\textit{Give a short outline of the scientific background of your research, including references.}\\

\textit{(about 1 page)}

We have developed a unique electronic structure code, 
called KKRnano \cite{zeller_towards_2008,thiess_massively_2012},
specifically designed for petaFLOP computing. Our method scales linearly
with the number of treated atoms so that we can realize system sizes of up to 
458752  atoms in a unit cell. This has already been shown on the full JUQUEEN
machine \cite{brommel_juqueen_2017}.
\\
Recently, we implemented a relativistic generalization of our algorithm 
enabling us to calculate complex non-collinear magnetic structures such as skyrmions
in real space. Skyrmions are two-dimensional magnetization solitons, i.e. two-dimensional
magnetic structures localized in space, topologically protected by a non-trivial
magnetization texture, which has particle-like properties. 
These days, they constitute one of the hottest subjects in the field of 
magnetism because such topological solitons are considered to be the smallest 
information-carrying particles at room temperature \cite{fert_skyrmions_2013}. 
\\
We plan to perform large-scale density functional theory (DFT) calculations with 
KKRnano for materials exhibiting these complex magnetic skyrmion textures on Hazel Hen 
since test runs on this particular machine confirmed our expectations with regards to performance. 
The focus of our work is on the germanide MnGe as it exhibits a three-dimensional magnetic structure,
which is not understood so far (see preliminary results
\cite{tanigaki_real-space_2015,bornemann_investigation_2017}).

\section{Preliminary Work}
\rule{\textwidth}{0.4pt}\\
\textit{Provide a brief summary of your preliminary work in connection with the proposed project, including references.}\\

\textit{(about 1 to 2 pages)}

KKRnano was first applied to Gd doped GaN where special emphasis was put on
magnetic interactions between defects \cite{thiess_superparamagnetism_2012}. Subsequently,
the phase change material GeSbTe was investigated \cite{zhang_role_2012}.
\\
During the last two years we have completely modernized our code and considerably improved 
its flexibility with respect to the number of cores and the number of MPI tasks and
OpenMP threads which can used in production runs. By changing the computational algorithms
we have also improved the computational efficiency both for conventional
supercomputers (e.g. JUQUEEN) and the cluster-booster architecture
(e.g. NVIDIA Tesla GPUs) \cite{dutot_addressing_2016}. 
We have tested the new version of the code for a number of large systems, 
in particular for MnGe supercells with B20 structure containing between 8192 and 458752 atoms,
during the JUQUEEN Extreme Scaling Workshop 2017 \cite{brommel_juqueen_2017}.
\\
As a first step towards unraveling the mystery of skyrmionic textures in B20-MnGe we
performed simulations based on simple micromagnetic models that can provide an elementary understanding
of magnetism in many-body systems. The results from this are discussed below.

\section{Description of the Project}
\rule{\textwidth}{0.4pt}\\
\subsection{Project Details}
%\textit{Describe your research project in detail, structured in sub-projects, if applicable. Include discussion of the scientific questions that you are planning to address and the overall scientific goals of the project. It is important that you describe the innovative aspects, impact and topicality of the proposal.}
%\begin{itshape}
%\begin{itemize}\setlength{\itemsep}{-2pt}
%  \item Scientific questions you want to address
%  \item Scientific objectives
%  \item Computational objectives
%  \item Approach and expected outcome
%  \item Expected impact on the research area
%  \item Scientific and technical innovation potential
%  \item Progress beyond the state-of-the-art
%\end{itemize}
%\end{itshape}

%\subsubsection{Sub-project 1}
%\textit{...}

%\subsubsection{Sub-project 2}
%\textit{...}\\

\textit{(1 to 2 pages per sub-project)}

\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		  \centering
		  \includegraphics[width=.99\linewidth]{Figures/tokura_exp.png}
		  %\caption{Enlarged in-plane magnetic-moment configurations as observed in experiment.}
		  \label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		  \centering
		  \includegraphics[width=.99\linewidth]{Figures/tokura_sim.png}
		  %\caption{Simulated in-plane magnetic-moment configurations and projected
		  %positions of hedgehogs (blue) and antihedgehogs (red).}
                  \label{fig:sub2}
	\end{subfigure}
	\caption{In-plane magnetic moment configurations of cubic-lattice skyrmion 
		in MnGe and projected positions of hedgehog and antihedgehog according to
	        Tanigaki et al. \cite{tanigaki_real-space_2015}.
		Left: Enlarged in-plane magnetic-moment configurations as observed in experiment.
		Right: Simulated in-plane magnetic-moment configurations and projected
		positions of magnetic hedgehogs (blue) and antihedgehogs (red).}
		
	\label{fig:tokura_results}
\end{figure}


This proposal is inspired by the recent discovery of magnetic skyrmion lattices in bulk helical magnets.
In these systems a variety of phenomena can be of importance to what is observed in experiment,
e.g. the topological Hall effect and spin-transfer torque at ultra-low current densities
\cite{kanazawa_large_2011}.
Special attention has been drawn on cubic B20-type compounds with broken lattice inversion symmetry.
These compounds form a class of materials in which skyrmion phases have been observed experimentally
\cite{nagaosa_topological_2013}.
In a recent study \cite{tanigaki_real-space_2015} 
it was found by real-space observation transmission electron microscopy 
that a cubic lattice of skyrmions and anti-skyrmions (see \cref{fig:tokura_results}) exists in B20-MnGe with
a lattice constant of about 3-6 nm. Findings by Kanazawa et al. suggest that this
lattice is set up by a superposition of three orthogonal helical
structures also referred to as 3q-state \cite{kanazawa_noncentrosymmetric_2017}.
In contrast to other systems exhibiting a similar magnetic phase,
here the rather short helical wavelength allows for first-principles density functional (DFT) calculations
with KKRnano. 
\\
MnGe is currently subject to extensive investigation in the solid state physics community
\cite{kanazawa_large_2011,kanazawa_possible_2012,grigoriev_chiral_2013,tanigaki_real-space_2015,
martin_magnetic_2016}.
However, to the best of our knowledge large-scale DFT calculations have not yet been performed.
We are confident that such calculations can provide new insight into the nature of
the observed magnetic textures and the underlying creation mechanisms.
At present there is a lack of a convincing explanation of what is observed in experiment. 
Research in the framework of micromagnetic models identified both magnetic frustration (RKKY interaction) 
as well as spin-orbit coupling induced Dzyaloshinskii-Moriya interaction as potentially
crucial to a better understanding \cite{koretsune_control_2015,altynbaev_hidden_2016}.
Our own spin model simulations which are less elaborate than a DFT-based approach with KKRnano yielded meta-stable
topologically protected skyrmionic textures \cite{bornemann_investigation_2017} stabilized by RKKY interaction
(see \cref{fig:sd_results}).
\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		  \centering
		  \includegraphics[width=.99\linewidth]{Figures/MnGe_skyrmion.png}
		  %\caption{}
		  \label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		  \centering
		  \includegraphics[width=.99\linewidth]{Figures/MnGe_antiskyrmion.png}
	          %\caption{}
                  \label{fig:sub2}
	\end{subfigure}
	\caption{Graphical representation of a Skyrmion (left) and an anti-skyrmion (right) 
	with atom magnetization pointing in positive (red) and negative (blue) z-direction. Results
	are obtained from a micromagnetic model with parameters for B20-MnGe
	taken from electronic structure calculations.}
	\label{fig:sd_results}
\end{figure}

Based on these simulations we also deem it
possible that textures even more exotic than skyrmions and anti-skyrmions \cite{rybakov_new_2015}
can be stabilized in B20-MnGe.
\\
In the past months first KKRnano calculations for B20-MnGe were conducted on the HPC machines JUQUEEN,
JURECA and QPACE3. A 6x6x6 supercell of MnGe, which contains 1728 atoms, is large enough to cover 
an edge length of 3 nm, and small enough for routine calculations with our code. Since one cannot 
expect the skyrmion lattice period, which we hope to find in DFT calculations, to be exactly the 
one observed in the experiments, we also plan to increase the size of the supercell to a 12x12x12
supercell with 13824 atoms and a length of 6 nm. If necessary we can increase the size even
to a 24x24x24 supercell with 110592 atoms \cite{brommel_juqueen_2017}.
\\
Our agenda is composed of the following sub-projects that we consider to be of importance
to a depper understanding of helical textures in
MnGe B20.

\subsubsection{Identification of non-trivial magnetic ground states}
The by far most important task in a large-scale DFT calculation for MnGe on which 
all the following sub-projects depend is the search for
non-trivial magnetic ground states as opposed to trivial ones, e.g. ferromagnetism or anti-ferromagnetism.
Here, our top priority is to simulate the aforementioned 3q-state. In addition the simulation of single stable
skyrmions and anti-skyrmions is planned. Especially anti-skyrmions are of great interest to us as they
have seldom if ever been observed in any material.
\\
The calculations planned here are going to require a lot of parameter tuning
(e.g. supercell size) and testing to find
suitable configurations and therefore
\textbf{we consider an allocation of 20 million core hours as necessary for this sub-project.}
\subsubsection{Creation mechanisms leading to non-trivial magnetic states}
The identification of ground-state candidates is to be followed by an investigation of
mechanisms that lead to the formation of such ground-states. Our all-electron approach will allow 
us to exactly quantify the origin of differences in total energy and therefore give us strong hints
to what the system-inherent physics are driven by.
\\
\textbf{We estimate a need for 10 million core hours for this sub-project.}
\subsubsection{Investigation of Mn(1-x)Fe(x)Ge alloys}
Both B20-MnGe and B20-FeGe are chiral magnets exhibiting helical structures. While in
B20-MnGe a suprisingly short helical wavelength of 3-6 nm is observed \cite{tanigaki_real-space_2015},
a much larger one of
approx. 70 nm is seen in B20-FeGe \cite{lebech_magnetic_1989}. This huge difference has not yet been explained.
Our hypothesis is that it originates from fundamentally different effects, i.e. that
magnetic frustration dominates in B20-MnGe while in B20-FeGe the interplay between
isotropic exchange interaction and Dzyaloshinskii-Moriya interaction plays a major role.
\\
We plan to
simulate Mn(1-x)Fe(x)Ge alloys where the concentration of Mn and Fe is varied.
This work is going to help us solve the question of what is the fundamental difference between
chiral magnetism in B20-MnGe and B20-FeGe.
\\
\textbf{10 million core hours are accounted for this sub-project.}

\subsubsection{Influence of impurities and defects}
B20-MnGe samples used to gather experimental data inevitably contain impurites and defects. 
These may have an influence on the magnetic properties of materials and thus play a crucial role
in the creation of helical structures. 
Here, our ambition is to simulate point defects, e.g. oxygen interstitials, in B20-MnGe and study
their impact.
\\
\textbf{10 million core hours are accounted for this sub-project.}

\subsection{Review Processes}
\textit{Has the underlying research project already successfully undergone a scientific review process? Is the project funded by public money?
If yes, please also provide information about the funding source
(e.g. State, BMWi, BMBF, DFG, EU, $\dots$)}

Projects based on KKRnano were
granted a substantial amount of computing time by several committees in the past years.
The code itself was awarded the High-Q club membership in 2014 which requires the ability to
effeciently utilise the entire 28-rack BlueGene/Q system installed in Jülich.


%The development of the KKR method in general and of the code KKRnano in particular has been recognized 
%by the Board of Directors of Forschungszentrum J{\"u}lich as important work in the interest of 
%Forschungszentrum J{\"u}lich. The Board of Directors has supported the work by a Helmholtz professorship 
%for Prof. Peter Dederichs, as the third Jülich scientist to be honored with the award,
%after the Nobel Prize winner Prof. Peter Grünberg in 2007 and Prof. Siegfried Mantl in 2011. 
%The Board of Directors has also supported the work by authorizing the employment of Dr. Rudolf Zeller, 
%who reached the mandatory retirement age in Oct. 2014, as a part-time employee at least until Nov. 2017. 


\section{Numerical Methods and Algorithms} 
\rule{\textwidth}{0.4pt}\\
\textit{Describe the numerical methods and algorithms that you are planning to use, improve, or develop.}\\
 
\textit{(1 to 2 pages)}
\bigskip

Contrary to other density-functional codes, which determine the Kohn-Sham orbitals by solving
the differential Kohn-Sham wave equation, in KKRnano the Green function for the Kohn-Sham
equation is obtained by solving an integral equation in real space. For the solution, space is divided
into non-overlapping cells around the atoms and the calculations are separated into single-cell parts,
where only the potential within the individual cell enters, and a large complex linear matrix equation for 
the matrix elements $G_{LL}^{nn'} (\epsilon)$ of the Green function $G(r, r , \epsilon)$:
\begin{equation}
	G_{LL'}^{nn'} (\epsilon) = G_{LL'}^{r,nn'} (\epsilon) + \sum_{n''} \sum_{L''}
	G_{LL''}^{r,nn''} (\epsilon) \sum_{L'''} \Delta t_{L'' L'''}^{n''} (\epsilon)
	G_{L'''L'}^{n''n'} (\epsilon)
	\label{eq:dyson_eq}
\end{equation}
In the following we refer to $\Delta t_{L'' L'''}^{n''} (\epsilon)$ as the $\Delta t$-matrices and
$G_{LL''}^{r,nn''} (\epsilon)$ as the reference Green functions. At this point we do not
discuss the formalism of obtaining them but refer the reader to the existing literature\cite{zeller_towards_2008}.
\\
The given problem is identical to solving a complex linear matrix equation of the form
\begin{equation}
	\matr{A} \vec{x} = \vec{b}
\end{equation}
As is well known this problem scales as $O(N^3)$, where $N$ denotes the dimension of the $N \times N$
matrix $\matr{A}$. Standard solving schemes therefore severly limit the treatable system size.

However, by choosing a reference system of repulsive potentials
$\matr{A}$ can be made sparse and this can again be exploited
by using customized iterative algorithms for solving linear matrix equations involving sparse matrices.
Our method of choice is the transpose free quasi minimal residual (tfQMR) algorithm \cite{freund_qmr:_1991}
which enables us to achieve $O(N^2)$ scaling. 
Convergence of the tfQMR iterations is achieved by working in the complex energy plane. Both concepts,
the repulsive reference system and application of complex energy have been
introduced by us into the KKR method \cite{zeller_application_1982,zeller_theory_1995}
and are used worldwide since many years. 
By truncating inter-atomic interactions above a certain distance even an $O(N)$ scaling 
behavior can be realized and this feature should be used in any calculation involving more than 1000 atoms.
In most application scenarios the tfQMR solver accounts for the major part of the computational work.
\\
For super-large-scale calculations (above 100000 atoms) the electrostatics solver begins
to require an amount of computing resources that is not negligible anymore. 
The electrostatic problem is given in terms of a Poisson equation that connects electric field and potential. 
Solving this equation scales quadratically with system size. 


\section{Computer Resources}
\rule{\textwidth}{0.4pt}\\
\subsection{Code performance and workflow}
%\textit{Describe the codes, packages or libraries that you need to undertake the project, and how these will enable the research to be achieved. Include for \textbf{each code to be used} information about}
%\begin{itshape}
%\begin{itemize}\setlength{\itemsep}{-2pt}
%  \item Which code will be used
%  \item How is the code parallelized (pure MPI, mixed MPI/OpenMP, Pthreads, CUDA, etc.)
%  \item The amount of memory necessary (per core, per node and in total)
%  \item Scaling plots \textbf{and} tables with speedup results for runs with typical, parameter sets, problem size, and I/O \textbf{of the planned project} (no general benchmark results are accepted)
%  \item Describe architecture, machine/system name, and problem size used for the scaling plots
%  \item Current job profile (independent jobs, chained jobs, workflow, etc.)
%\end{itemize}
%\end{itshape}

The natural parallelization strategy in KKRnano is domain decomposition with a simple 
matching between atomic cells and processors. Both, the single scattering part of the 
calculations and the iterative solution of \cref{eq:dyson_eq} can be done separately for each cell. 
Communication between the processors is required to distribute the
$\Delta t$-matrices and the 
reference Green function matrix
before \cref{eq:dyson_eq} is solved and because density information
must be distributed to obtain the electrostatic potential for the next density functional
self-consistency step. KKRnano is also parallelized over the energy points $\epsilon$ and 
the two spin directions so that more than one MPI process per atom can be utilized.
On the other hand it is also possible to map several atoms to one MPI process so that
systems with several thousand atoms can be investigated also if only a few hundred
processors are available, however, with associated longer wall-clock runtimes.
\\
For an efficient use of computers featuring symmetric multiprocessing (SMP) KKRnano contains openMP directives
in the most time consuming parts so that e.g. on JUQUEEN calculations can be performed with combinations 
ranging from 1 MPI task and 64 threads to 64 MPI tasks and 1 thread per node. Weak-scaling data
for different combinations
obtained on JUQUEEN is depicted in \cref{fig:total_runtimes}.
\begin{figure}[h]
\begin{center}
 \includegraphics[scale=0.45]{Figures/total_runtimes.pdf}
\end{center}
\caption{Runtime of one self-consistent iteration with KKRnano 
	on IBM BlueGene/Q at Forschungszentrum J{\"u}lich.
	This data was obtained with a problem size ranging from 8192 atoms (1 rack) to 229376 atoms (28 racks).
	Different distribution of MPI tasks (MPI) and openMP threads (omp) leads to slightly different runtimes.}
\label{fig:total_runtimes}
\end{figure}
For these performance tests MnGe supercells
ranging from 8192 atoms (1 rack) to 229376 atoms (28 racks/full machine) were used.
In order to achieve linear scaling in the tfQMR solver the calculations were 
done by setting the interaction range described by truncation of the Green function to include 
always 1308 atoms around each considered site.
\cref{fig:tfqmr_es_times} provides a scaling analysis of the tfQMR and electrostatics solver.
\begin{figure}[h]
\begin{center}
 \includegraphics[scale=0.45]{Figures/combinedtfqmrelectrostatics.pdf}
\end{center}
\caption{Time per iteration spent in the tfQMR solver (solid lines) 
	and in the electrostatics solver (dashed lines) for the calculations depicted in
	\cref{fig:total_runtimes}.}
\label{fig:tfqmr_es_times}
\end{figure}
It nicely shows 
a constant runtime of the tfQMR solver but also that the runtime of the electrostatics solver becomes
more and more significant, when calculations are done for more than 65536 atoms (8 racks). 
However, the standard system sizes we are aiming
for with this project proposal are going to be smaller than that.
The small discrepancy between \cref{fig:total_runtimes} and \cref{fig:tfqmr_es_times} with regards to linear
scaling can be mainly attributed to Fortran direct-access I/O which does not scale well on a GPFS file system.
The implementation of a more suitable I/O library (e.g., MPI I/O or SIONlib) is likely to solve this issue.
In terms of memory consumption it could be observed that the 1GB/core ratio on JUQUEEN is not exceeded.
Additionally, test runs on Hazel Hen showed that the memory provided per node is sufficient for the calculations
we aim for.
\\
KKRnano jobs are launched as independent jobs requiring a short (less than 60 seconds) serial 'prepare'-step on
the head node
before launching the parallel large-scale calculation. All post-processing will be
performed on local computers.

%{\it \textbf{Important:} please take into account the corresponding technical guidelines and requirements (e.g. required minimal code scalability, memory restictions, etc.) of the machine you have chosen!}\\

%\textit{Here we give an example table and plot for presenting scaling and performance information.}

%\begin{table}[H]
%\caption{\label{tab_scaling} Scaling behavior of {\tt $<$code$>$} on {\tt $<$architecture and system$>$} at {\tt $<$location$>$}. This test was performed with $5\cdot 10^6$ particles, absolute timings per timestep (s) and relative speedup normalized to 1024 cores are given.}
%\begin{center} 
%\setlength{\tabcolsep}{10pt}
%\renewcommand{\arraystretch}{1.2}
% \begin{tabular}{lcccc}
% \#cores  & absolute timing (s) & speedup  & Performance per core [MFLOP/s]\\
%\hline
%1024     & 189.6                        & 1.0000   & 600 \\
%2048     & \phantom{1}99.0      & 1.9154   & 576 \\
%4096     & \phantom{1}55.6      & 3.4088   & 511 \\
%8192     & \phantom{1}30.8      & 6.1376   & 460 \\

%\end{tabular}
%\end{center}
%\end{table}


%\begin{table}[h!]
%	\caption{Scaling behaviour of KKRnano on JUQUEEN at Forschungszentrum J{\"u}lich. 
%		Total runtime (Total), tfQMR solver runtime (tfQMR) and electrostatics solver runtime (ES)
%		    in weak scaling measurement series employing moderate OpenMP parallelization
%		      with $16$ MPI ranks per node and $4$ threads per process. 
%		        Runtimes are given in seconds. (t) indicates the tuned version where final I/O is omitted.}
%			\begin{center}
%				\begin{tabular}{|c|r|r|r|r|r|r|}
%					\hline
%					 Supercell               & bg\_size & MPI ranks &   Threads &    Total & tfQMR &  ES \\
%					\hline\hline
%					$16 \times  8 \times  8$ &  1,024   &  16,384   &    65,536 &      750 &   432 &  84 \\
%					$16 \times 16 \times  8$ &  2,048   &  32,768   &   131,072 &      839 &   432 &  86 \\
%					$16 \times 16 \times 16$ &  4,096   &  65,536   &   262,144 &      973 &   430 &  96 \\
%					$32 \times 16 \times 16$ &  8,192   & 131,072   &   524,288 &     1223 &   431 & 113 \\
%					$32 \times 32 \times 16$ & 16,384   & 262,144   & 1,048,576 &     1880 &   432 & 196 \\
%					$32 \times 32 \times 24$ & 24,576   & 393,216   & 1,572,864 & (t) 1077 &   431 & 353 \\
%					$32 \times 32 \times 28$ & 28,672   & 458,752   & 1,835,008 & (t) 1210 &   430 & 470 \\
%					\hline
%				\end{tabular}
%			\end{center}
%			\label{kkrnano:mnge_weakscaling_moderateomp}
%		\end{table}





\textit{(1 to 2 pages)}
\newpage
\subsection{Justification of resources requested}
%\textit{Outline the amount of resources you request for the current granting period, structured in sub-projects, if applicable. This should include information such as}
%\begin{itshape}
%\begin{itemize}\setlength{\itemsep}{-2pt}
%    \item Type of run (e.g. pre- /post-processing run, production run, visualization, etc.)

%    \item Problem size for planned runs (e.g. \# particles or the like)
%    \item Number of runs planned
%    \item Number of steps per run
%    \item Wall-clock time per run
%    \item Number of cores/GPUs used per run
%    \item Total amount of requested computing time (core-hours and/or GPU-hours, if applicable)
%    \item Resources for data analytics, if applicable
%\end{itemize}
%\end{itshape}

We plan to do pure production runs that feature different non-trivial magnetic textures, e.g. skyrmions.
Each run consists of typically about 100 self-consistent iterations that are needed to converge
the electronic density. Our current setup requires approximately 1000 s per iteration depending on the accuracy
the calculation aims at. We stress that such calculations depend on several tunable parameters, e.g. number of
k-points, cutoff radii and atomic positions. Therefore, it is essential to check results with different parameter
settings in order to make sure that physical interpretations are valid and reproducable. 
With the \textbf{requested amount of 50 million core hours} (approx $1.8 \times 10^{11}$ core-s)
we are able to do roughly 1000 independent calculations with
the smallest setup featuring a unit cell of 1728 atoms:
\begin{equation}
	n_{\text{calculations}} = \frac{1.8 \times 10^{11} }{ 1000 \times 100 \times 1728} \approx 1000 
\end{equation}
For the larger calculations computational needs increase linearly, e.g. a setup of
13824 atoms allows for 125 full calculations given 50 million core hours on Hazel Hen.
Note that
experimental data suggests that single magnetic textures can be stabilized in supercells containing between
1728 and 13824 atoms which corresponds to a length scale of 3-6 nm.

%\textit{This information should take the form of a table like the example table shown below. Please, specify the requested time in appropriate units, preferred units are core hours (core-h) and GPU-hours.}\\



%\begin{tabular}{llcccccc} \hline\hline
%  Sub-project & 
%  Type &
%  Problem & 
%  \# runs & 
%  \# steps/ & 
%  Wall time/ & 
%  \# cores/ & 
%  Total \\
%  &
%  of run &
%  size  &
%  &
%  run &
%  step [hours] &
%  run &
%  [core-h] \\
% \hline\hline
%  Sub-proj. 1 & 
%  Preproc. &
%  P1 & 
%  R1 & 
%  S1 &
%  W1 &
%  C1 &
%  R1$\cdot$S1$\cdot$W1$\cdot$C1 \\
%   ~     &
%  Type 1 &
%  P2 & 
%  R2 & 
%  S2 &
%  W2 &
%  C2 &
%  R2$\cdot$S2$\cdot$W2$\cdot$C2 \\
%$\cdots$ &\multicolumn{7}{c}{$\cdots$}\\
%\hline\hline
%TOTAL & & & & & & & sum of above\\
%\end{tabular}
%\bigskip

\textit{(0.5 to 1 page)}

\section{Resource Management and Work Schedule}
\rule{\textwidth}{0.4pt}\\
%\subsection{Resource management}
%\textit{Describe how you intend to manage the resources you have requested. This should include a description of the methods you will deploy to monitor progress of the project and how project results are documented.}\\

\textit{(0.5 to 1 page)}

%\subsection{Work schedule}
%\textit{Please, provide a short work schedule, structured in sub-projects, if applicable. Include a table and/or Gantt chart.}

%\subsubsection{Sub-project 1}
%\textit{...}

%\subsubsection{Sub-project 2}
%\textit{...}\\

The progress of the work concerned with the development and the application of KKRnano
is discussed in regular bi-weekly strategy meetings of the project team.
The code development and the protocols of the strategy meetings are documented
using the Trac system at URL https://trac.version.fz-juelich.de/KKRnano so
that all changes to the code and all experience with its use are easy to find.


\bigskip
%\textit{Example for a Gantt chart:}
%\begin{figure}[H]
%\begin{flushleft}
% \includegraphics[scale=0.82]{Figures/gantt_chart.jpg}
%\end{flushleft}
% \caption{\label{fig_workschedule}Work schedule for the project.}
%\end{figure}

\section{Key Personnel and Experiences}
\rule{\textwidth}{0.4pt}\\
\textit{Give a short introduction of the key persons involved in the project and their experience (max 3 persons).}\\

\textit{(half a page)}

Prof. Stefan Bl{\"u}gel is...
\\
Marcel Bornemann is a PhD student, who has already
obtained a detailed knowledge of the computer code KKRnano and of the principles of writing modern code. 
He was able to implement important enhancements in KKRnano as the extension to non-collinear
magnetism including relativistic effects without much help from the senior scientists.
\\
Dr. Rudolf Zeller has been working in the field of density-functional calculations for more than 40 years. 
He is the main developer of the full-potential KKR programs, including KKRnano, 
which are now used in J{\"u}lich and several other places in Germany. 
\\
Dr. Paul F. Baumeister is...
\\
Prof. Peter H. Dederichs is one of the pioneers in electronic structure work in Germany.
From 2004 to 2012 he was chairman of the Psi-k network, a European-based, worldwide network 
of researchers working on the advancement of first-principles computational materials science.
\newpage

\bibliography{My_Library}

%\section{Bibliographic References}
%\rule{\textwidth}{0.4pt}\\
%\textit{Provide recent/most important bibliographic references that are relevant to the project.}\\

\bigskip
\begin{flushright}
{\tiny V1.3-2016JUN07}
\end{flushright}
\end{document}
